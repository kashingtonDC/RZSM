{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import ee\n",
    "import numpy as np\n",
    "import geopandas as gp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import rsfuncs as rs\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.signal import savgol_filter\n",
    "from datetime import datetime, timedelta\n",
    "from datetime import datetime as dt\n",
    "from itertools import chain\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [18,16]\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EE functions are in the rsfuncs module. Loacal functions are here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sm_file(filename):\n",
    "    with open(filename) as f:\n",
    "        contents = f.readlines()\n",
    "\n",
    "    data = []\n",
    "    for line in contents:\n",
    "        if line[0:1] == \"#\":\n",
    "            continue\n",
    "        else:\n",
    "            data.append(line)\n",
    "\n",
    "    headers = [x.replace(\"Soil Moisture Percent\",\"smp\").replace(\" \",\"_\") for x in data[0].split(\",\")]\n",
    "    cols = [x.strip(\"\\n\").split(\",\") for x in data[1:]]\n",
    "\n",
    "    df = pd.DataFrame(cols, columns = headers)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def imlist_2_timeseries(imlist,polarization, area):\n",
    "    \n",
    "    '''\n",
    "    This essemtially combines `get_ims_by_date` and `array from latlon` functions below \n",
    "    '''\n",
    "    num_ims = len(imlist.getInfo())\n",
    "\n",
    "    ims = []\n",
    "    dates = []\n",
    "    \n",
    "    for idx in range (0, num_ims):\n",
    "        latlon = ee.Image.pixelLonLat().addBands(imlist.get(idx))\n",
    "        res = latlon.reduceRegion(reducer=ee.Reducer.toList(),geometry=area,maxPixels=1e8,scale=10)\n",
    "        \n",
    "        try:\n",
    "            lats = np.array((ee.Array(res.get(\"latitude\")).getInfo()))\n",
    "            lons = np.array((ee.Array(res.get(\"longitude\")).getInfo()))\n",
    "            data = np.array((ee.Array(res.get(polarization)).getInfo()))\n",
    "        except:\n",
    "            data = np.full_like(lats, np.nan,dtype=np.float64)\n",
    "\n",
    "        im = make_np_array(data, lats, lons)\n",
    "        ims.append(im)\n",
    "        \n",
    "        date =  latlon.get('system:time_start')\n",
    "        info_dict = imlist.get(i).getInfo()\n",
    "        date = info_dict['id']\n",
    "        dates.append(date)\n",
    "        \n",
    "    return ims, dates\n",
    "\n",
    "def get_ims_by_date(ims_list, var, res=10):\n",
    "    imlist = []\n",
    "    imdates = []\n",
    "    num_images = len(ims_list.getInfo())\n",
    "\n",
    "    for i in range (0, num_images):\n",
    "        \n",
    "        if i % 5 == 0:\n",
    "            print(str((i / num_images)*100)[:5] + \" % \")\n",
    "\n",
    "        latlon = ee.Image.pixelLonLat().addBands(ims_list.get(i))\n",
    "        imlist.append(array_from_latlon(latlon, var, res))\n",
    "        date =  latlon.get('system:time_start')\n",
    "        info_dict = ims_list.get(i).getInfo()\n",
    "        date = info_dict['id']\n",
    "        imdates.append(date)\n",
    "  \n",
    "    return imlist, imdates\n",
    "\n",
    "def array_from_latlon(latlon_obj, var, res):\n",
    "    res = latlon_obj.reduceRegion(reducer=ee.Reducer.toList(),geometry=area,maxPixels=1e8,scale=res)\n",
    "    try:\n",
    "        lats = np.array((ee.Array(res.get(\"latitude\")).getInfo()))\n",
    "        lons = np.array((ee.Array(res.get(\"longitude\")).getInfo()))\n",
    "        data = np.array((ee.Array(res.get(var)).getInfo()))\n",
    "    except:\n",
    "        data = np.full_like(lats, np.nan,dtype=np.float64)\n",
    "    \n",
    "    out = make_np_array(data, lats, lons)\n",
    "    return out   \n",
    "\n",
    "def make_np_array(data, lats, lons):\n",
    "    # get data from df as arrays\n",
    "    lons = np.array(lons)\n",
    "    lats = np.array(lats)\n",
    "    data = np.array(data) # Set var here \n",
    "                                              \n",
    "    # get the unique coordinates\n",
    "    uniqueLats = np.unique(lats)\n",
    "    uniqueLons = np.unique(lons)\n",
    "\n",
    "    # get number of columns and rows from coordinates\n",
    "    ncols = len(uniqueLons)    \n",
    "    nrows = len(uniqueLats)\n",
    "\n",
    "    # determine pixelsizes\n",
    "    ys = uniqueLats[1] - uniqueLats[0] \n",
    "    xs = uniqueLons[1] - uniqueLons[0]\n",
    "\n",
    "    # create an array with dimensions of image\n",
    "    arr = np.zeros([nrows, ncols], np.float32)\n",
    "\n",
    "    # fill the array with values\n",
    "    counter =0\n",
    "    for y in range(0,len(arr),1):\n",
    "        for x in range(0,len(arr[0]),1):\n",
    "            if lats[counter] == uniqueLats[y] and lons[counter] == uniqueLons[x] and counter < len(lats)-1:\n",
    "                counter+=1\n",
    "                arr[len(uniqueLats)-1-y,x] = data[counter] # we start from lower left corner\n",
    "    \n",
    "    return arr\n",
    "\n",
    "def filter_date(product,y,m,d):\n",
    "    start = ee.Date.fromYMD(y,m,d).advance(-1, \"day\")\n",
    "    end = ee.Date.fromYMD(y,m,d)\n",
    "    prod = product.filterDate(start, end).sort('system:time_start', False).select(\"ppt\")\n",
    "    return prod\n",
    "\n",
    "def format_dates(dates):\n",
    "    for idx, x in enumerate(dates):\n",
    "        timestamp = x.find(\"V_\")+2\n",
    "        timestr = x[timestamp:timestamp+13]\n",
    "        dates[idx] = pd.to_datetime(timestr, format='%Y%m%d %H:%M')\n",
    "    return dates\n",
    "\n",
    "def get_2day_precip(latlon_obj, area):\n",
    "    res = latlon_obj.reduceRegion(reducer=ee.Reducer.sum(),geometry=area,scale=10)\n",
    "    data = np.array((ee.Array(res.get(\"ppt\")).getInfo()))\n",
    "    out = np.array(data)\n",
    "    return out \n",
    "\n",
    "def get_ndvi(latlon_obj, area):\n",
    "    res = latlon_obj.reduceRegion(reducer=ee.Reducer.mean(),geometry=area,scale=10)\n",
    "    data = np.array((ee.Array(res.get(\"NDVI\")).getInfo()))\n",
    "    out = np.array(data)\n",
    "    return out \n",
    "\n",
    "def round_up_to_odd(f):\n",
    "    f = int(np.ceil(f))\n",
    "    return f + 1 if f % 2 == 0 else f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_file = gp.read_file(\"../shape/scan_sites.shp\")\n",
    "sites = site_file[~site_file['state'].isin([\"AK\", \"HI\", \"PR\", \"VI\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = rs.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processings site no 2181\n",
      "Cultivated Crops - Areas used for the production of annual crops, such as corn, soybeans, vegetables, tobacco, and cotton, and also perennial woody crops such as orchards and vineyards. Crop vegetation accounts for greater than 20 percent of total vegetation. This class also includes all land being actively tilled.\n",
      "Processing 4 VV sentinel overapasses\n",
      "0.0 % \n",
      "Processing 4 HV sentinel overapasses\n",
      "0.0 % \n",
      "processing PRISM\n",
      "0.0 % \n",
      "Processing Landsat\n",
      "0.0 % \n",
      "2.183 % \n",
      "4.366 % \n",
      "6.550 % \n",
      "8.733 % \n",
      "10.91 % \n",
      "13.10 % \n",
      "15.28 % \n",
      "17.46 % \n",
      "19.65 % \n",
      "21.83 % \n",
      "24.01 % \n",
      "26.20 % \n",
      "28.38 % \n",
      "30.56 % \n",
      "32.75 % \n",
      "34.93 % \n",
      "37.11 % \n",
      "39.30 % \n"
     ]
    }
   ],
   "source": [
    "# For each site id, find the sm file with the data (in data dir), \n",
    "# calculate psi as psi = A * SM ^b (krishna's paper )\n",
    "# query the (1) landcover, (2) Sentinel backscatter (Prism P), MODIS / Landsat LAI for the whole timeseries \n",
    "   \n",
    "for idx, row in sites[10:].iterrows():\n",
    "    \n",
    "    if row.id in out_dict.keys():\n",
    "        print(row.id)\n",
    "        continue\n",
    "        \n",
    "    print(\"Processings site no {}\".format(row.id))\n",
    "    \n",
    "    # Make geom to submit to EE \n",
    "    x,y = row.geometry.buffer(0.0001).envelope.exterior.coords.xy\n",
    "    coords = [list(zip(x,y))]\n",
    "    area = ee.Geometry.Polygon(coords)\n",
    "    \n",
    "    # Get the corresponding SCAN data file from data folder\n",
    "    site_id = row.id\n",
    "    sm_file = [os.path.join(data_dir,x) for x in os.listdir(data_dir) if site_id in x][0]\n",
    "    sm_dat = read_sm_file(sm_file)\n",
    "    sm_dat['Date'] =  pd.to_datetime(sm_dat['Date'], format='%Y%m%d %H:%M')\n",
    "    sm_dat.set_index('Date', inplace=True)\n",
    "        \n",
    "    # start and end date\n",
    "    if sm_dat.empty:\n",
    "        print(\"no valid soil moisture data for {}\".format(row.id))\n",
    "        continue\n",
    "        \n",
    "    startdate = sm_dat.index[0]\n",
    "    enddate = sm_dat.index[-1]\n",
    "    \n",
    "    date = startdate.strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "    # Select the nlcd dataset\n",
    "    dataset =  rs.load_data()['nlcd']\n",
    "    ic = dataset[0]\n",
    "    var = dataset[1]\n",
    "    res = dataset[3]\n",
    "    \n",
    "    # find the nearest nlcd dataset\n",
    "    col = ic.filterDate(ee.Date(date).advance(-1, 'years'), ee.Date(date).advance(2, 'years')).first()\n",
    "    if not col.getInfo():\n",
    "        col = ic.filterDate(ee.Date(date).advance(-3, 'years'), ee.Date(date).advance(2, 'years')).first()\n",
    "    t = col.reduceRegion(ee.Reducer.frequencyHistogram(), area, res).get(var)\n",
    "    meta = col.getInfo()\n",
    "    lc_class = int(list(t.getInfo().keys())[0])\n",
    "    \n",
    "    # Get the landcover type\n",
    "    lcidx = meta['properties']['landcover_class_values'].index(lc_class)\n",
    "    lctype = meta['properties']['landcover_class_names'][lcidx]\n",
    "    print(lctype)\n",
    "    \n",
    "    # Get Sentinel images and dates (descending orbits only, VV, HV polarization)\n",
    "    s1 = rs.load_data()['s1']\n",
    "    s1ic, s1var, s1res = s1[0], s1[1], s1[3]\n",
    "    \n",
    "    # Krishna used ascending pass... I think descending is the correct orbit for the AM \n",
    "\n",
    "    col = s1ic.filterBounds(area).filter(ee.Filter.eq('orbitProperties_pass', 'DESCENDING')).select(s1var).filterDate(startdate,enddate).sort('system:time_start')\n",
    "    vv = col.filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV'))\n",
    "    hv = col.filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VH'))\n",
    "    \n",
    "    # Fetch the S1 data \n",
    "    try:\n",
    "        vv_ims = vv.toList(vv.size())\n",
    "        print(\"Processing {} VV sentinel overapasses\".format(len(vv_ims.getInfo())))\n",
    "        s1_vv, vv_dates = get_ims_by_date(vv_ims,\"VV\")\n",
    "    except:\n",
    "        print(\"no valid VV overpasses\")\n",
    "        s1_vv = []\n",
    "        vv_dates = []\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        hv_ims = hv.toList(hv.size())\n",
    "        print(\"Processing {} HV sentinel overapasses\".format(len(hv_ims.getInfo())))\n",
    "        s1_hv, hv_dates = get_ims_by_date(hv_ims,\"VH\")\n",
    "    except:\n",
    "        print(\"no valid HV overpasses\")\n",
    "        s1_hv = []\n",
    "        hv_dates = []\n",
    "        pass \n",
    "    \n",
    "    if len(s1_hv) == 0 and len(s1_vv) == 0:\n",
    "        print(\"No valid sentinel data \")\n",
    "        continue\n",
    "    \n",
    "    # Convert the datestrings from S1 to pandas datetimes \n",
    "       \n",
    "    vvdates = format_dates(vv_dates)\n",
    "    hvdates = format_dates(hv_dates)\n",
    "    \n",
    "    \n",
    "    s1_vv = [x.flatten() for x in s1_vv]\n",
    "    s1_hv = [x.flatten() for x in s1_hv]\n",
    "    \n",
    "    vvdf = pd.DataFrame(list(dict(zip(vv_dates, s1_vv)).items()),\n",
    "                      columns=['t','vv'])\n",
    "    hvdf = pd.DataFrame(list(dict(zip(vv_dates, s1_hv)).items()),\n",
    "                      columns=['t','hv'])\n",
    "        \n",
    "    # Get PRISM data for all the S1 overpass dates to filter the rainy days\n",
    "    print(\"processing PRISM\")\n",
    "    rainfall = []\n",
    "\n",
    "    for i,x in enumerate(vv_dates):\n",
    "        if i % 5 == 0:\n",
    "            print(str((i / len(vv_dates))*100)[:5] + \" % \")\n",
    "        \n",
    "        y,m,d = vv_dates[i].year, vv_dates[i].month, vv_dates[i].day\n",
    "        t = filter_date(rs.load_data()['prism_daily'][0], y, m, d).sum()\n",
    "        precip_total = get_2day_precip(t, area)\n",
    "        rainfall.append(precip_total)\n",
    "\n",
    "                \n",
    "    # Landsat - Note: some sites are in the overlap areas between passes.\n",
    "    # these sites can have multiple obs / day or obs separated by 8days instead of 16. \n",
    "    \n",
    "    print(\"Processing Landsat\")\n",
    "    landsat = rs.load_data()['l8_sr']\n",
    "    lic, lvar, lsf = landsat[0],landsat[1], landsat[2]\n",
    "\n",
    "    lstart = ee.Date.fromYMD(startdate.year,startdate.month,startdate.day).advance(-9,\"day\")\n",
    "    lend = ee.Date.fromYMD(enddate.year,enddate.month,enddate.day).advance(8, \"day\")\n",
    "\n",
    "    l8_col = lic.filterDate(lstart,lend).filterBounds(area).map(rs.mask_quality) # Mask clouds and shadows \n",
    "    lt = l8_col.sort('system:time_start')\n",
    "    lims = lt.toList(lt.size())\n",
    "\n",
    "    num_ims = len(lims.getInfo())\n",
    "\n",
    "    ldfs = []\n",
    "\n",
    "    for i in range (0, num_ims):\n",
    "        if i % 5 == 0:\n",
    "            print(str((i / num_ims)*100)[:5] + \" % \")\n",
    "\n",
    "        ls_latlon = ee.Image.pixelLonLat().addBands(lims.get(i))\n",
    "        ltemp = ls_latlon.select([\"B1\",\"B2\",\"B3\",\"B4\",\"B5\",\"B6\",\"B7\"]).multiply(lsf)\n",
    "        l8_res = ltemp.reduceRegion(reducer=ee.Reducer.mean(),geometry=area,bestEffort=True,scale=30)\n",
    "\n",
    "        l8_info_dict = lims.get(i).getInfo()\n",
    "        l8_date = l8_info_dict['id'][-8:]\n",
    "\n",
    "        l8_out = l8_res.getInfo()\n",
    "\n",
    "        ldf = pd.DataFrame.from_dict(l8_out.values()).T\n",
    "        ldf.columns = l8_out.keys()\n",
    "        ldf.index = pd.to_datetime([l8_date])\n",
    "        ldfs.append(ldf)\n",
    "        \n",
    "                        \n",
    "    # Filter the SCAN data for the S1 dates, use 3 am - 7 am mean \n",
    "    rzsm = []\n",
    "    ssm = []\n",
    "    \n",
    "    for i in vv_dates:\n",
    "        starttime = i.replace(second=0, microsecond=0, minute=0, hour=3)\n",
    "        endtime = starttime+timedelta(hours= 4)\n",
    "        df = pd.DataFrame(sm_dat[starttime:endtime])\n",
    "        \n",
    "        rzsm.append(df[df.columns[-1]].values)\n",
    "        ssm.append(df[df.columns[-3]].values)\n",
    "    \n",
    "    # In case there are nans or data gaps in the sm data\n",
    "    rzsm = [list(filter(None, x)) for x in rzsm]\n",
    "    ssm = [list(filter(None, x)) for x in ssm]\n",
    "    \n",
    "    # Calc the 5 hour mean for each sensor reading \n",
    "    for idx,x in enumerate(rzsm):\n",
    "        rzsm[idx] = np.nanmean([np.float(i) for i in x])\n",
    "    \n",
    "    for idx,x in enumerate(ssm):\n",
    "        ssm[idx] = np.nanmean([np.float(i) for i in x])\n",
    "\n",
    "    # params to calculate psi \n",
    "    a = row.a\n",
    "    b = row.b\n",
    "    \n",
    "    # RZ and Surface LWP = A * sm ^b\n",
    "    psi1 = [a*(x/100)**b for x in rzsm]\n",
    "    psi2 = [a*(x/100)**b for x in ssm]\n",
    "    \n",
    "    # Make a dataframe out of everything\n",
    "    df = pd.DataFrame([vv_dates, mean_sigmas, std_sigmas, rainfall, modis_lai, rzsm,psi1,ssm,psi2, [lcidx]*len(dates)])\n",
    "    df = df.T\n",
    "    df.columns = (['date', \"sigma\", \"std_sigma\", \"precip\",\"LAI\", \"rzsm\",\"psi_rz\", \"ssm\", \"psi_s\", \"lc_type\"])\n",
    "    df = rs.col_to_dt(df) # set the date col as datetime index \n",
    "    dfout = df.copy()\n",
    "    \n",
    "    # Drop non rainy overpasses from df \n",
    "#     mask=(df['precip'] < 0.1)\n",
    "#     dfout = df[mask]\n",
    "\n",
    "    # Golay interpolation for landsat \n",
    "    \n",
    "    ls = pd.concat(ldfs)\n",
    "    win_len = round_up_to_odd(num_ims/7)\n",
    "    \n",
    "    for i in ls.columns:\n",
    "        ls[i][ls[i] == 0] = np.nan\n",
    "        ls[i] = ls[i].interpolate(method = \"linear\")\n",
    "        ls[i+\"_filt\"] = savgol_filter(ls[i], window_length=win_len, polyorder=2)\n",
    "    \n",
    "    ls_df = ls[(ls.T != 0).any()]\n",
    "    ls_df = ls_df.groupby(level = 0).mean()\n",
    "    \n",
    "    # Break the loop if there are no non- rainy sentinel overpasses \n",
    "    if dfout.empty:\n",
    "        print(\"No non-rainy overpaasses \")\n",
    "        continue \n",
    "        \n",
    "    l8_dfs = []\n",
    "    \n",
    "    for i in dfout.index:\n",
    "        l8_idx = ls_df.index.get_loc(i, method='nearest')\n",
    "        l8_contemp = ls_df.iloc[l8_idx]\n",
    "        t = pd.DataFrame(l8_contemp).T\n",
    "        l8_dfs.append(t)\n",
    "    \n",
    "    fin_ls = pd.concat(l8_dfs)\n",
    "\n",
    "    FIN = pd.concat( [dfout.reset_index(drop=True), fin_ls.reset_index(drop=True)], axis=1) \n",
    "    FIN.index = dfout.index\n",
    "\n",
    "    print(FIN.head())\n",
    "    \n",
    "    out_dict[(row.id)] = FIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.concat(out_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.to_csv(\"../data/all_dat_desc.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
