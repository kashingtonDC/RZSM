{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/aakashahamed/anaconda3/envs/gis/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/aakashahamed/anaconda3/envs/gis/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/aakashahamed/anaconda3/envs/gis/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/aakashahamed/anaconda3/envs/gis/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/aakashahamed/anaconda3/envs/gis/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/aakashahamed/anaconda3/envs/gis/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import glob\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import rsfuncs as rs\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from pandas.tseries.offsets import MonthEnd, SemiMonthEnd\n",
    "from datetime import datetime, timedelta\n",
    "from datetime import datetime as dt\n",
    "\n",
    "from keras import regularizers, optimizers\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "states_file = gp.read_file(\"../shape/states.shp\")\n",
    "states = states_file[~states_file['STATE_ABBR'].isin([\"AK\", \"HI\", \"PR\", \"VI\"])]\n",
    "\n",
    "site_file = gp.read_file(\"../shape/scan_sites.shp\")\n",
    "sites = site_file[~site_file['state'].isin([\"AK\", \"HI\", \"PR\", \"VI\"])]\n",
    "\n",
    "# formate data \n",
    "df = pd.read_csv(\"../data/all_dat_f.csv\")\n",
    "df.rename(columns={ df.columns[0]: \"site\" , df.columns[1]:\"date\"}, inplace = True)\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace = True)\n",
    "df = df.set_index(pd.to_datetime(df.date))\n",
    "df['date'] = pd.to_datetime(df.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data\n",
    "\n",
    "# Drop days with 2day precip less than 1 mm\n",
    "df = df[df.precip<1]\n",
    "\n",
    "# Remove Urban Areas\n",
    "df = df[df.lc_type != 2]\n",
    "df = df[df.lc_type != 3]\n",
    "\n",
    "# Remove sites with <10 datapoints\n",
    "for i in df.site.unique():\n",
    "    if len(df[df.site == i]) < 10:\n",
    "        df = df[df.site != i]\n",
    "\n",
    "# Calculate spectral indices\n",
    "# df['ndvi'] = (df.B5 - df.B4) / (df.B5 + df.B4)\n",
    "# df[\"ndmi\"] = (df.B5 - df.B6) / (df.B5 + df.B6)\n",
    "# df[\"evi\"] = 2.5*(df.B5 - df.B4) / (df.B5 + 6*df.B4- 7.5*df.B2 + 1)\n",
    "\n",
    "df['ndvi'] = (df.B5_filt - df.B4_filt) / (df.B5_filt + df.B4_filt)\n",
    "df[\"ndmi\"] = (df.B5_filt - df.B6_filt) / (df.B5_filt + df.B6_filt)\n",
    "df[\"evi\"] = 2.5*(df.B5_filt - df.B4_filt) / (df.B5_filt + 6*df.B4_filt - 7.5*df.B2_filt + 1)\n",
    "\n",
    "# For the backscatter columns (df.vv, df.hv), delete any zeros, nans, deal with weird formatting, and calc the mean \n",
    "vv_eff = []\n",
    "\n",
    "for i in df.vv:\n",
    "    line = i.replace(\"[\",\"\")\n",
    "    line = line.replace(\"]\",\"\")\n",
    "    line = ' '.join(line.split())\n",
    "    data = [float(i) for i in line.split(' ')]\n",
    "    data = [i for i in data if i !=0.]\n",
    "    vv_eff.append(np.nanmean(data))\n",
    "    \n",
    "\n",
    "hv_eff = []\n",
    "\n",
    "for i in df.hv:\n",
    "    if type(i) is float:\n",
    "        hv_eff.append(np.nan)\n",
    "    else:\n",
    "        line = i.replace(\"[\",\"\")\n",
    "        line = line.replace(\"]\",\"\")\n",
    "        line = ' '.join(line.split())\n",
    "        data = [float(i) for i in line.split(' ')]\n",
    "        data = [i for i in data if i !=0.]\n",
    "        hv_eff.append(np.nanmean(data))\n",
    "\n",
    "\n",
    "df['vv'] = vv_eff\n",
    "df['hv'] = hv_eff\n",
    "\n",
    "# calc the 12 day means for each site: \n",
    "df = df.groupby(['site']).resample('12D').mean().fillna(np.nan).dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the non interpolated landsat bands\n",
    "df.drop([\"B1\", \"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], axis = 1,inplace = True)\n",
    "df.rename(columns={'B1_filt': 'B1', 'B2_filt': 'B2','B3_filt': 'B3','B4_filt': 'B4','B5_filt': 'B5','B6_filt': 'B6','B7_filt': 'B7'}, inplace=True)\n",
    "\n",
    "# cols = [c for c in df.columns if 'filt' not in c]\n",
    "# df=df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode the landcover types *** figure this out\n",
    "# df = df.lc_type.astype(str)\n",
    "one_hot = pd.get_dummies(df.lc_type, drop_first=True )\n",
    "rdf = pd.concat([df, one_hot], axis = 1)\n",
    "rdf = rdf.drop([\"lc_type\"], axis = 1)\n",
    "# df = rdf.reset_index(level='site')\n",
    "df = rdf.dropna()\n",
    "df.columns = df.columns.map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the time lagged predictors \n",
    "s1 = df.groupby(level=\"site\").shift(1)\n",
    "df1 = df.join(s1.rename(columns=lambda x: x+\"_t1\"))\n",
    "\n",
    "s2 = df.groupby(level=\"site\").shift(2)\n",
    "df2 = df1.join(s2.rename(columns=lambda x: x+\"_t2\"))\n",
    "\n",
    "df3 = df2.copy()\n",
    "# s3 = df.groupby(level=\"site\").shift(3)\n",
    "# df3 = df2.join(s3.rename(columns=lambda x: x+\"_t3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary cols "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropstrs = ['site','precip', 'rzsm_', 'ssm', 'psi_rz', \"psi_s\"]\n",
    "dropcols = []\n",
    "\n",
    "for i in df3.columns:\n",
    "    for x in dropstrs:\n",
    "        if x in i:\n",
    "            dropcols.append(i)\n",
    "            \n",
    "df3 = df3.drop(dropcols, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index and drop nans \n",
    "df = df3.reset_index(level='site')\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling options\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCHSIZE = 50\n",
    "DROPOUT = 0.25\n",
    "LOSS = 'mse'\n",
    "\n",
    "Areg = regularizers.l1(0.00005)\n",
    "Breg = regularizers.l2(0.001)\n",
    "Kreg = regularizers.l2(1e-15)\n",
    "Rreg = regularizers.l2(1e-15)\n",
    "\n",
    "def build_model(input_shape):\n",
    "    # make the model \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(50, input_shape=input_shape, dropout = 0.05,recurrent_dropout=0.05,return_sequences=True,bias_regularizer= Breg))\n",
    "    model.add(LSTM(25, input_shape=input_shape, dropout = 0.05,recurrent_dropout=0.05,return_sequences=True,bias_regularizer= Breg))\n",
    "    model.add(LSTM(5, input_shape=input_shape, dropout = 0.05,recurrent_dropout=0.05,bias_regularizer= Breg))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss=LOSS, optimizer= \"Nadam\",\n",
    "                  metrics=['mse'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = {\"train_preds\":[], # rs\n",
    "       \"test_preds\":[], # wells\n",
    "       \"train_act\":[], # c2vsim\n",
    "       \"test_act\":[]  # grace \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2145\n",
      "WARNING:tensorflow:From /Users/aakashahamed/anaconda3/envs/gis/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/aakashahamed/anaconda3/envs/gis/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 2945 samples, validate on 44 samples\n",
      "Epoch 1/500\n",
      " - 1s - loss: 1.0777 - mse: 0.9978 - val_loss: 0.6901 - val_mse: 0.6107\n",
      "Epoch 2/500\n",
      " - 0s - loss: 1.0624 - mse: 0.9830 - val_loss: 0.7014 - val_mse: 0.6224\n",
      "Epoch 3/500\n",
      " - 0s - loss: 1.0488 - mse: 0.9699 - val_loss: 0.7214 - val_mse: 0.6429\n",
      "Epoch 4/500\n",
      " - 0s - loss: 1.0298 - mse: 0.9513 - val_loss: 0.7466 - val_mse: 0.6686\n",
      "Epoch 5/500\n",
      " - 0s - loss: 1.0093 - mse: 0.9313 - val_loss: 0.7862 - val_mse: 0.7086\n",
      "Epoch 6/500\n",
      " - 0s - loss: 0.9827 - mse: 0.9052 - val_loss: 0.8613 - val_mse: 0.7841\n",
      "Epoch 7/500\n",
      " - 0s - loss: 0.9491 - mse: 0.8721 - val_loss: 0.9503 - val_mse: 0.8736\n",
      "Epoch 8/500\n",
      " - 0s - loss: 0.9144 - mse: 0.8379 - val_loss: 1.0600 - val_mse: 0.9838\n",
      "Epoch 9/500\n",
      " - 0s - loss: 0.8786 - mse: 0.8025 - val_loss: 1.1756 - val_mse: 1.0998\n",
      "Epoch 10/500\n",
      " - 0s - loss: 0.8432 - mse: 0.7676 - val_loss: 1.2932 - val_mse: 1.2179\n",
      "Epoch 11/500\n",
      " - 0s - loss: 0.8180 - mse: 0.7429 - val_loss: 1.1487 - val_mse: 1.0739\n",
      "Epoch 12/500\n",
      " - 0s - loss: 0.7982 - mse: 0.7235 - val_loss: 1.4243 - val_mse: 1.3502\n",
      "Epoch 13/500\n",
      " - 0s - loss: 0.7714 - mse: 0.6973 - val_loss: 1.5133 - val_mse: 1.4397\n",
      "Epoch 14/500\n",
      " - 0s - loss: 0.7631 - mse: 0.6896 - val_loss: 1.7523 - val_mse: 1.6792\n",
      "Epoch 15/500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-e22afab64861>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m     train_history = model.fit(X_train, y_train,epochs = 500,\n\u001b[1;32m     41\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                     verbose=2, validation_data=(X_test, y_test))\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gis/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/gis/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gis/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/anaconda3/envs/gis/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in random.sample(set(df.site.unique()), len(df.site.unique())):\n",
    "\n",
    "    print(\"Processing {}\".format(i))\n",
    "    \n",
    "    # Hold one out cross validation - loop through sites and select 1 to test on and the rest to train \n",
    "    sdf = df[df.site == i]\n",
    "    y_test = sdf.rzsm\n",
    "    X_test = sdf.drop([\"site\",\"rzsm\"], axis=1)\n",
    "\n",
    "    nsdf = df[df.site != i]\n",
    "    y_train = nsdf.rzsm \n",
    "    X_train = nsdf.drop([\"site\",\"rzsm\"], axis=1)\n",
    "        \n",
    "    # Scale data\n",
    "    transformer_x = StandardScaler().fit(X_train)\n",
    "    transformer_y = StandardScaler().fit(np.array(y_train).reshape(-1, 1)) \n",
    "    \n",
    "#     transformer_x = MinMaxScaler(feature_range=(0,1)).fit(X_train)\n",
    "#     transformer_y = MinMaxScaler(feature_range=(0,1)).fit(np.array(y_train).reshape(-1, 1)) \n",
    "    \n",
    "    X_train = transformer_x.transform(X_train)\n",
    "    y_train = transformer_y.transform(np.array(y_train).reshape(-1, 1))\n",
    "    X_test = transformer_x.transform(X_test)\n",
    "    y_test = transformer_y.transform(np.array(y_test).reshape(-1, 1))\n",
    "    \n",
    "    # LSTM params\n",
    "    n_lags = 3\n",
    "    n_features = 19\n",
    "\n",
    "    X_train = X_train.reshape((X_train.shape[0], n_lags, 19))\n",
    "    X_test = X_test.reshape((X_test.shape[0], n_lags, 19))\n",
    "\n",
    "    input_shape=(X_train.shape[1], X_train.shape[2])\n",
    "    \n",
    "    model = build_model(input_shape=input_shape)\n",
    "    \n",
    "    # Fit\n",
    "#     model.fit(X_train, y_train, epochs=1000, batch_size=1000, verbose=1)\n",
    "    \n",
    "    train_history = model.fit(X_train, y_train,epochs = 500,\n",
    "                    batch_size=2000,\n",
    "                    verbose=2, validation_data=(X_test, y_test))\n",
    "    loss = train_history.history['loss']\n",
    "    val_loss = train_history.history['val_loss']\n",
    "    val_acc = train_history.history['val_mse']\n",
    "\n",
    "    # Get the predictions\n",
    "    yhat_train = model.predict(X_train)\n",
    "    \n",
    "    trainPredict = transformer_y.inverse_transform(yhat_train.reshape(-1,1))\n",
    "    trainY = transformer_y.inverse_transform(y_train)\n",
    "    \n",
    "    yhat = model.predict(X_test)\n",
    "    testPredict = transformer_y.inverse_transform(yhat.reshape(-1,1))\n",
    "    testY = transformer_y.inverse_transform(y_test)\n",
    "\n",
    "    trainScore = math.sqrt(mean_squared_error(y_train, trainPredict))\n",
    "    print('Train Score: %.2f RMSE' % (trainScore))\n",
    "    testScore = math.sqrt(mean_squared_error(y_test, testPredict))\n",
    "    print('Test Score: %.2f RMSE' % (testScore))\n",
    "    \n",
    "    out[\"test_preds\"] = testPredict\n",
    "    out[\"train_preds\"] = trainPredict\n",
    "    \n",
    "    out[\"test_act\"] = testY\n",
    "    out[\"train_act\"] = trainY\n",
    "    \n",
    "    plt.plot(loss)\n",
    "    plt.plot(val_loss)\n",
    "    plt.legend(['loss', 'val_loss'])\n",
    "    plt.show()\n",
    "\n",
    "    plt.scatter(trainPredict, trainY)\n",
    "    plt.scatter(testPredict, testY)\n",
    "    plt.plot([0, 100], [0, 100], 'k-')\n",
    "    plt.title('''\n",
    "                train set RMSE = {}\n",
    "                test set RMSE = {}\n",
    "                '''.format(round(trainScore,2),round(testScore,2)))\n",
    "\n",
    "    plt.xlabel(\"predicted\")\n",
    "    plt.ylabel(\"actual\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2945, 3, 19)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "19*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#added some parameters\n",
    "kf = KFold(n_splits = 5, shuffle = True, random_state = 2)\n",
    "result = next(kf.split(df), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2391"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.iloc[result[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "598"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.iloc[result[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = kf.split(df.groupby(\"site\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object _BaseKFold.split at 0x1c59479620>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.site.unique():\n",
    "    sdf = df[df.site == i]\n",
    "    max_len = 0\n",
    "    if len(sdf)>max_len:\n",
    "        max_len = len(sdf)\n",
    "        answer = i\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
