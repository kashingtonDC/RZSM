{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/aakashahamed/anaconda3/envs/gis/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/aakashahamed/anaconda3/envs/gis/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/aakashahamed/anaconda3/envs/gis/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/aakashahamed/anaconda3/envs/gis/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/aakashahamed/anaconda3/envs/gis/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/aakashahamed/anaconda3/envs/gis/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import glob\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import rsfuncs as rs\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from pandas.tseries.offsets import MonthEnd, SemiMonthEnd\n",
    "from datetime import datetime, timedelta\n",
    "from datetime import datetime as dt\n",
    "\n",
    "from keras import regularizers, optimizers\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "states_file = gp.read_file(\"../shape/states.shp\")\n",
    "states = states_file[~states_file['STATE_ABBR'].isin([\"AK\", \"HI\", \"PR\", \"VI\"])]\n",
    "\n",
    "site_file = gp.read_file(\"../shape/scan_sites.shp\")\n",
    "sites = site_file[~site_file['state'].isin([\"AK\", \"HI\", \"PR\", \"VI\"])]\n",
    "\n",
    "# formate data \n",
    "df = pd.read_csv(\"../data/all_dat_f.csv\")\n",
    "df.rename(columns={ df.columns[0]: \"site\" , df.columns[1]:\"date\"}, inplace = True)\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace = True)\n",
    "df = df.set_index(pd.to_datetime(df.date))\n",
    "df['date'] = pd.to_datetime(df.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data\n",
    "\n",
    "# Drop days with 2day precip less than 1 mm\n",
    "df = df[df.precip<1]\n",
    "\n",
    "# Remove Urban Areas\n",
    "df = df[df.lc_type != 2]\n",
    "df = df[df.lc_type != 3]\n",
    "\n",
    "# Remove sites with <10 datapoints\n",
    "for i in df.site.unique():\n",
    "    if len(df[df.site == i]) < 10:\n",
    "        df = df[df.site != i]\n",
    "\n",
    "# Calculate spectral indices\n",
    "# df['ndvi'] = (df.B5 - df.B4) / (df.B5 + df.B4)\n",
    "# df[\"ndmi\"] = (df.B5 - df.B6) / (df.B5 + df.B6)\n",
    "# df[\"evi\"] = 2.5*(df.B5 - df.B4) / (df.B5 + 6*df.B4- 7.5*df.B2 + 1)\n",
    "\n",
    "df['ndvi'] = (df.B5_filt - df.B4_filt) / (df.B5_filt + df.B4_filt)\n",
    "df[\"ndmi\"] = (df.B5_filt - df.B6_filt) / (df.B5_filt + df.B6_filt)\n",
    "df[\"evi\"] = 2.5*(df.B5_filt - df.B4_filt) / (df.B5_filt + 6*df.B4_filt - 7.5*df.B2_filt + 1)\n",
    "\n",
    "# For the backscatter columns (df.vv, df.hv), delete any zeros, nans, deal with weird formatting, and calc the mean \n",
    "vv_eff = []\n",
    "\n",
    "for i in df.vv:\n",
    "    line = i.replace(\"[\",\"\")\n",
    "    line = line.replace(\"]\",\"\")\n",
    "    line = ' '.join(line.split())\n",
    "    data = [float(i) for i in line.split(' ')]\n",
    "    data = [i for i in data if i !=0.]\n",
    "    vv_eff.append(np.nanmean(data))\n",
    "    \n",
    "\n",
    "hv_eff = []\n",
    "\n",
    "for i in df.hv:\n",
    "    if type(i) is float:\n",
    "        hv_eff.append(np.nan)\n",
    "    else:\n",
    "        line = i.replace(\"[\",\"\")\n",
    "        line = line.replace(\"]\",\"\")\n",
    "        line = ' '.join(line.split())\n",
    "        data = [float(i) for i in line.split(' ')]\n",
    "        data = [i for i in data if i !=0.]\n",
    "        hv_eff.append(np.nanmean(data))\n",
    "\n",
    "\n",
    "df['vv'] = vv_eff\n",
    "df['hv'] = hv_eff\n",
    "\n",
    "# calc the 12 day means for each site: \n",
    "df = df.groupby(['site']).resample('12D').mean().fillna(np.nan).dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the non interpolated landsat bands\n",
    "df.drop([\"B1\", \"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\"], axis = 1,inplace = True)\n",
    "df.rename(columns={'B1_filt': 'B1', 'B2_filt': 'B2','B3_filt': 'B3','B4_filt': 'B4','B5_filt': 'B5','B6_filt': 'B6','B7_filt': 'B7'}, inplace=True)\n",
    "\n",
    "# cols = [c for c in df.columns if 'filt' not in c]\n",
    "# df=df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode the landcover types\n",
    "# df = df.lc_type.astype(str)\n",
    "one_hot = pd.get_dummies(df.lc_type, drop_first=True )\n",
    "rdf = pd.concat([df, one_hot], axis = 1)\n",
    "rdf = rdf.drop([\"lc_type\"], axis = 1)\n",
    "# df = rdf.reset_index(level='site')\n",
    "df = rdf.dropna()\n",
    "df.columns = df.columns.map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the time lagged predictors \n",
    "s1 = df.groupby(level=\"site\").shift(1)\n",
    "df1 = df.join(s1.rename(columns=lambda x: x+\"_t1\"))\n",
    "\n",
    "s2 = df.groupby(level=\"site\").shift(2)\n",
    "df2 = df1.join(s2.rename(columns=lambda x: x+\"_t2\"))\n",
    "\n",
    "df3 = df2.copy()\n",
    "# s3 = df.groupby(level=\"site\").shift(3)\n",
    "# df3 = df2.join(s3.rename(columns=lambda x: x+\"_t3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary cols "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropstrs = ['site','precip', 'rzsm_', 'ssm', 'psi_rz', \"psi_s\"]\n",
    "dropcols = []\n",
    "\n",
    "for i in df3.columns:\n",
    "    for x in dropstrs:\n",
    "        if x in i:\n",
    "            dropcols.append(i)\n",
    "            \n",
    "df3 = df3.drop(dropcols, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index and drop nans \n",
    "df = df3.reset_index(level='site')\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling options\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCHSIZE = 50\n",
    "DROPOUT = 0.25\n",
    "LOSS = 'mse'\n",
    "\n",
    "Areg = regularizers.l1(0.00005)\n",
    "Breg = regularizers.l2(0.001)\n",
    "Kreg = regularizers.l2(1e-15)\n",
    "Rreg = regularizers.l2(1e-15)\n",
    "\n",
    "def build_model(input_shape):\n",
    "    # make the model \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(50, input_shape=input_shape, dropout = 0.05,recurrent_dropout=0.05,return_sequences=True,bias_regularizer= Breg))\n",
    "    model.add(LSTM(25, input_shape=input_shape, dropout = 0.05,recurrent_dropout=0.05,return_sequences=True,bias_regularizer= Breg))\n",
    "    model.add(LSTM(5, input_shape=input_shape, dropout = 0.05,recurrent_dropout=0.05,bias_regularizer= Breg))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss=LOSS, optimizer= \"Nadam\",\n",
    "                  metrics=['mse'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = {\"train_preds\":[], # rs\n",
    "       \"test_preds\":[], # wells\n",
    "       \"train_act\":[], # c2vsim\n",
    "       \"test_act\":[]  # grace \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2183.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 71116 into shape (3092,3,19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e22afab64861>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m19\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_lags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m19\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_lags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m19\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 71116 into shape (3092,3,19)"
     ]
    }
   ],
   "source": [
    "for i in random.sample(set(df.site.unique()), len(df.site.unique())):\n",
    "\n",
    "    print(\"Processing {}\".format(i))\n",
    "    \n",
    "    # Hold one out cross validation - loop through sites and select 1 to test on and the rest to train \n",
    "    sdf = df[df.site == i]\n",
    "    y_test = sdf.rzsm\n",
    "    X_test = sdf.drop([\"site\",\"rzsm\"], axis=1)\n",
    "\n",
    "    nsdf = df[df.site != i]\n",
    "    y_train = nsdf.rzsm \n",
    "    X_train = nsdf.drop([\"site\",\"rzsm\"], axis=1)\n",
    "        \n",
    "    # Scale data\n",
    "    transformer_x = StandardScaler().fit(X_train)\n",
    "    transformer_y = StandardScaler().fit(np.array(y_train).reshape(-1, 1)) \n",
    "    \n",
    "#     transformer_x = MinMaxScaler(feature_range=(0,1)).fit(X_train)\n",
    "#     transformer_y = MinMaxScaler(feature_range=(0,1)).fit(np.array(y_train).reshape(-1, 1)) \n",
    "    \n",
    "    X_train = transformer_x.transform(X_train)\n",
    "    y_train = transformer_y.transform(np.array(y_train).reshape(-1, 1))\n",
    "    X_test = transformer_x.transform(X_test)\n",
    "    y_test = transformer_y.transform(np.array(y_test).reshape(-1, 1))\n",
    "    \n",
    "    # LSTM params\n",
    "    n_lags = 3\n",
    "    n_features = 19\n",
    "\n",
    "    X_train = X_train.reshape((X_train.shape[0], n_lags, 19))\n",
    "    X_test = X_test.reshape((X_test.shape[0], n_lags, 19))\n",
    "\n",
    "    input_shape=(X_train.shape[1], X_train.shape[2])\n",
    "    \n",
    "    model = build_model(input_shape=input_shape)\n",
    "    \n",
    "    # Fit\n",
    "#     model.fit(X_train, y_train, epochs=1000, batch_size=1000, verbose=1)\n",
    "    \n",
    "    train_history = model.fit(X_train, y_train,epochs = 500,\n",
    "                    batch_size=2000,\n",
    "                    verbose=2, validation_data=(X_test, y_test))\n",
    "    loss = train_history.history['loss']\n",
    "    val_loss = train_history.history['val_loss']\n",
    "    val_acc = train_history.history['val_mse']\n",
    "\n",
    "    # Get the predictions\n",
    "    yhat_train = model.predict(X_train)\n",
    "    \n",
    "    trainPredict = transformer_y.inverse_transform(yhat_train.reshape(-1,1))\n",
    "    trainY = transformer_y.inverse_transform(y_train)\n",
    "    \n",
    "    yhat = model.predict(X_test)\n",
    "    testPredict = transformer_y.inverse_transform(yhat.reshape(-1,1))\n",
    "    testY = transformer_y.inverse_transform(y_test)\n",
    "\n",
    "    trainScore = math.sqrt(mean_squared_error(y_train, trainPredict))\n",
    "    print('Train Score: %.2f RMSE' % (trainScore))\n",
    "    testScore = math.sqrt(mean_squared_error(y_test, testPredict))\n",
    "    print('Test Score: %.2f RMSE' % (testScore))\n",
    "    \n",
    "    out[\"test_preds\"] = testPredict\n",
    "    out[\"train_preds\"] = trainPredict\n",
    "    \n",
    "    out[\"test_act\"] = testY\n",
    "    out[\"train_act\"] = trainY\n",
    "    \n",
    "    plt.plot(loss)\n",
    "    plt.plot(val_loss)\n",
    "    plt.legend(['loss', 'val_loss'])\n",
    "    plt.show()\n",
    "\n",
    "    plt.scatter(trainPredict, trainY)\n",
    "    plt.scatter(testPredict, testY)\n",
    "    plt.plot([0, 100], [0, 100], 'k-')\n",
    "    plt.title('''\n",
    "                train set RMSE = {}\n",
    "                test set RMSE = {}\n",
    "                '''.format(round(trainScore,2),round(testScore,2)))\n",
    "\n",
    "    plt.xlabel(\"predicted\")\n",
    "    plt.ylabel(\"actual\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58748"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3092*1*19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#added some parameters\n",
    "kf = KFold(n_splits = 5, shuffle = True, random_state = 2)\n",
    "result = next(kf.split(df), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2391"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.iloc[result[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "598"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.iloc[result[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = kf.split(df.groupby(\"site\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object _BaseKFold.split at 0x1c59479620>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.site.unique():\n",
    "    sdf = df[df.site == i]\n",
    "    max_len = 0\n",
    "    if len(sdf)>max_len:\n",
    "        max_len = len(sdf)\n",
    "        answer = i\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
